# brms_SummaryTable(model,
#                   formatOptions=list(digits=5),
#                   astrology=TRUE) #panderize=TRUE
if (modelName %in% names(allModels)){
warning("Model Name already exists!")
}
table =tidy_stan(model, prob=0.95)
plo =stanplot(model, type="areas",
pars ="^b_", prob=0.95,# pars ="b_[^I]", prob=0.95,
point_est="median")+
ggtitle("Posterior distributions",
"with medians and 95% quantile-based-intervals") +
theme(plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5))
print(table)
print(plo)
# stanplot(logitRandomIntercept, type="dens", pars ="b_")+
#   labs(title="Posterior distributions of parameters")+
#   theme(plot.title = element_text(hjust = 0.5))
# if(!is.null(allModels[[modelName]])){
#
# }
allModels[[modelName]]<<-model
}
#Other BRMS options:
# Here: save model to a list. Write the list at the end.
#estimate=mean (expvalue)
# LOO(fit1, fit2)
# plot(logitRandomIntercept)
# launch_shinystan(logitRandomIntercept)
#
# pp_check(logitRandomIntercept)
# stanplot(logitRandomIntercept, type="dens", pars ="b_")#dens, hist, areas
#
# summary(logitRandomIntercept)
# #sJPlot, snakecase
# #equi_test(logitRandomIntercept, out = "plot")
# tidy_stan(logitRandomIntercept, prob=0.95)
knitr::opts_chunk$set(echo = TRUE)
data_type="Work" #Directing to the right folder Raw Data or Work data
experiment="exp_v4"#"Exp" #e.g. "Pilot","Exp"
#versions=c("V2") #c("imV1")
project_folder = "/mnt/vol2/ICx3/"
code_folder = paste0(project_folder, "code/")
data_folder = paste0(project_folder, "data/")
project_folder = "~/Google Drive/My Drive/Shared PabloDN/ICx3/"
code_folder = paste0(project_folder, "Code/behavAnalysis/")
data_folder = paste0(project_folder, "Data/Behavioural Data/", experiment,"/", data_type,"/")
setwd(code_folder)
knitr::opts_knit$set(root.dir = code_folder)
#Import functions
source(file.path(code_folder,"DescriptiveFunctions.R"))
# Participant Data
#fileParticipants = paste0(project_folder,"Data/Participants Log.csv")
#3SAT Data Text files
#folderDataSAT=paste0(data_folder,"SAT/")
# TSP Data Text files
#folderDataTSP=paste0(data_folder,"TSP/")
folderOut = paste0(code_folder,"Output")
folderOut_figures = paste0(code_folder,"Output/Figures")
folderOut_tables = paste0(code_folder,"Output/Tables")
#Solver Data Text files
# # SAT Files
# folderIn_solver_SAT = "~/Google Drive/Melbourne/UNIMELB/Research/Complexity Project/Simulations Data/3-SAT/sat-results/"
# fileIn_solverP_SAT = paste0(folderIn_solver_SAT,"kd-5/minisat_sat_5p_results.csv")
# fileIn_solverE_SAT = paste0(folderIn_solver_SAT,"kd-5/minisat_sat_5e_results.csv")
#
# #TSP Files
# folderIn_solver_TSP ="~/Google Drive/Melbourne/UNIMELB/Research/Complexity Project/Simulations Data/TSP/exp-20-5/"
#
# fileIn_solver_TSP = paste0(folderIn_solver_TSP,"tsp-20-5.csv")
# folderInstanceInfo='~/Google Drive/Melbourne/UNIMELB/Research/Complexity Project/Simulations Data/'
# fileDecInstances = paste0(folderInstanceInfo,'KS decision/decisionInstancesInfoSubset.csv')
# fileOptInstances = paste0(folderInstanceInfo,'KS optimisation/optimisationInstancesInfoSubset.csv')
#
# fileDecInstancesAll = paste0(folderInstanceInfo,'KS decision/decisionInstancesInfo.csv')
#
# optTaskMaxTime=60
SATTaskMaxTime=110
nVars_sat=5
TSPTaskMaxTime = 40
## Setting up the basics
library(ggplot2)
#library(stargazer)
library(knitr)
library(dplyr)
library(ggsignif)
library(pander)
library(plotly)
library(reshape2)
library(Hmisc)
library(readr)
library(brms)
library(parallel)
#library(lazerhawk)#devtools::install_github('m-clark/lazerhawk')#for table with brms sumamry
library(sjstats)
library(bayesplot)
library(bmlm)
library(qgraph)
library(DiagrammeR)
library(tidyr)
#Stores a list of all the regressions that are run
allModels=vector("list", length=0)
#Output type for tables. Use "html" to view the output .html file and use "latex" to export .tex tables
outputType="html"
chains_brms = 4
cores_brms = min(chains_brms,detectCores())
seed_brms = 111
save_summarise_model = function(model, modelName){
# brms_SummaryTable(model,
#                   formatOptions=list(digits=5),
#                   astrology=TRUE) #panderize=TRUE
if (modelName %in% names(allModels)){
warning("Model Name already exists!")
}
table =tidy_stan(model, prob=0.95)
plo =stanplot(model, type="areas",
pars ="^b_", prob=0.95,# pars ="b_[^I]", prob=0.95,
point_est="median")+
ggtitle("Posterior distributions",
"with medians and 95% quantile-based-intervals") +
theme(plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5))
print(table)
print(plo)
# stanplot(logitRandomIntercept, type="dens", pars ="b_")+
#   labs(title="Posterior distributions of parameters")+
#   theme(plot.title = element_text(hjust = 0.5))
# if(!is.null(allModels[[modelName]])){
#
# }
allModels[[modelName]]<<-model
}
#Other BRMS options:
# Here: save model to a list. Write the list at the end.
#estimate=mean (expvalue)
# LOO(fit1, fit2)
# plot(logitRandomIntercept)
# launch_shinystan(logitRandomIntercept)
#
# pp_check(logitRandomIntercept)
# stanplot(logitRandomIntercept, type="dens", pars ="b_")#dens, hist, areas
#
# summary(logitRandomIntercept)
# #sJPlot, snakecase
# #equi_test(logitRandomIntercept, out = "plot")
# tidy_stan(logitRandomIntercept, prob=0.95)
detectCores()
calculate_sat_ICexpost =function(min_cost,nclauses){
ICexpost  =  min_cost/nclauses
return(ICexpost)
}
dataTrial_SAT= read_csv2(file.path(data_folder,"SAT_clean.csv"))
dataTrial_SAT_Time = read_csv2(file.path(data_folder,"SAT_clean_time.csv"))
dataclicks = read_csv2(file.path(data_folder,"SAT_clicks.csv"))
color_scheme_set("green")
dataTrial_SAT$censored_time = ifelse(dataTrial_SAT$timeSpent == SATTaskMaxTime,
"right","none")
dataTrial_SAT_Time$censored_time = ifelse(dataTrial_SAT_Time$timeSpent == SATTaskMaxTime,
"right","none")
instanceProperties_SAT= read_rds(paste0(data_folder,"instance_properties_SAT.rds"))
instanceProperties_SAT = instanceProperties_SAT %>% select(instanceNumber, min_cost, min_model,costs,models,nSolutions)
dataTrial_SAT = left_join(dataTrial_SAT,instanceProperties_SAT, by = "instanceNumber")
dataTrial_SAT_Time = left_join(dataTrial_SAT_Time,instanceProperties_SAT, by = "instanceNumber")
dataTrial_SAT$ICexpost = calculate_sat_ICexpost(dataTrial_SAT$min_cost,dataTrial_SAT$nClauses)
dataTrial_SAT_Time$ICexpost = calculate_sat_ICexpost(dataTrial_SAT_Time$min_cost,dataTrial_SAT_Time$nClauses)
dataInput=dataTrial_SAT
#dataInput=nSolutions_sat(dataInput)
#Plots nSolutions (x) vs. Accuracy (y)
dataInput3 = dataInput %>% group_by(nSolutions,pID,phaseT)%>%summarise(accuracyMeans=mean(correct))%>%ungroup()%>%group_by(nSolutions,phaseT)%>%summarise(accuracy=mean(accuracyMeans),se=se(accuracyMeans))%>%ungroup()
dataInput3$sol = dataInput3$nSolutions>=1
dataInput3$phaseT = recode(dataInput3$phaseT, '0' = "Low IC", '1' = "High IC")
plo= ggplot(data=dataInput3, aes(y=accuracy, x=as.factor(nSolutions))) +
geom_errorbar(aes(ymin=accuracy-se, ymax=accuracy+se), width=.1)+
geom_point(shape=23, size=3, fill="red")+
labs(title="Accuracy and the Number of Solutions",
x="Number of   Solutions",y="Accuracy")+
coord_cartesian(ylim = c(0.5,1))+
theme_light()+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
plot.title = element_text(hjust = 0.5),strip.text.x = element_blank())+
facet_grid(phaseT~sol, scales = "free_x", space = "free_x")+
geom_smooth(data=dataInput3, aes(y=accuracy, x=nSolutions),method=glm,se=FALSE,fullrange=TRUE,linetype = "dashed")
outputName = 'sat_acc_05_g'
plotExport(plo,outputName,folderOut_figures)
#Number of solutions of solvable solutions: Meand difference between In/Out of Phase T
dataInput2 = unique(dataInput %>% select(phaseT,id,nSolutions,sol) %>% filter(sol==1))
diffMeans = t.test(nSolutions ~ phaseT ,data=dataInput2)
pander(diffMeans)
# dataInput$phaseT = as.factor(dataInput$phaseT)
# dataInput$sol = as.factor(dataInput$sol)
dataInput = dataInput %>% filter(sol==1)
#Includes a dummy if nSolutions==0, that's variable: sol.
logitRandomIntercept = brm(correct ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
family=bernoulli(link="logit"),
data=dataInput,
chains=chains_brms,
cores = cores_brms,                            seed=seed_brms, refresh = 0)
tableName='sat_acc_05_r_A'
save_summarise_model(logitRandomIntercept, tableName)
#This version recodes phaseT to OutphaseT, to
fit = hypothesis(logitRandomIntercept,"nSolutions + phaseT:nSolutions = 0", seed=seed_brms)
print(fit)
hdi(fit$samples,prob=0.95)
#This version recodes phaseT to OutphaseT, to
fit = hypothesis(logitRandomIntercept,"nSolutions + phaseT:nSolutions = 0", seed=seed_brms)
print(fit)
hdi(fit$samples,prob=0.95)
#This version recodes phaseT to OutphaseT, to
fit = hypothesis(logitRandomIntercept,"nSolutions + phaseT:nSolutions = 0", seed=seed_brms)
print(fit)
hdi(fit$samples,prob=0.95)
#marginal_effects(logitRandomIntercept, ask=FALSE)
plot(marginal_effects(logitRandomIntercept), plot = TRUE, ask = FALSE)
dataInput = dataTrial_SAT
# Summarise the data to plot
mean_accuracy = dataInput %>%
group_by(instanceNumber, sol, ICexpost, phaseT, nSolutions) %>%
summarise(accuracy = mean(correct))
ggplot(mean_accuracy, aes(x = ICexpost, y = accuracy))+
geom_point() +
theme_light() +
stat_smooth(formula =  y ~ I(x^0.01), method="lm", se= FALSE)+
xlab("IC_expost")+
ylab("Human Accuracy")
dataInput = dataInput %>% filter(sol==0)
model_ICexpost = brm(correct ~ ICexpost + (1|pID),
family=bernoulli(link="logit"),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms,
refresh = 0)
tableName='sat_acc_icexpost'
save_summarise_model(model_ICexpost, tableName)
dataInput=dataTrial_SAT_Time
#Plots nSolutions (x) vs. Accuracy (y)
dataInput3 = dataInput %>% group_by(nSolutions,pID,phaseT)%>%summarise(timeSpent1=mean(timeSpent))%>%ungroup()%>%group_by(nSolutions,phaseT)%>%summarise(timeSpent=mean(timeSpent1),se=se(timeSpent1))%>%ungroup()
dataInput3$sol = dataInput3$nSolutions>=1
dataInput3$phaseT = recode(dataInput3$phaseT, '0' = "Low IC", '1' = "High IC")
plo= ggplot(data=dataInput3, aes(y=timeSpent, x=as.factor(nSolutions))) +
geom_errorbar(aes(ymin=timeSpent-se, ymax=timeSpent+se), width=.1)+
geom_point(shape=23, size=3, fill="red")+
labs(title="Time Spent and the Number of Solutions",
x="Number of   Solutions",y="Time Spent")+
#coord_cartesian(ylim = c(0.5,1))+
theme_light()+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
plot.title = element_text(hjust = 0.5),strip.text.x = element_blank())+
facet_grid(phaseT~sol, scales = "free_x", space = "free_x")+
geom_smooth(data=dataInput3, aes(y=timeSpent, x=nSolutions),method=glm,
se=FALSE,fullrange=TRUE,linetype = "dashed")
outputName = "sat_time_05_g"
plotExport(plo,outputName,folderOut_figures)
#dataInput$phaseT = as.factor(dataInput$phaseT)
dataInput = dataInput %>% filter(sol==1)
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
randomIntercept = brm(timeSpent ~ phaseT + nSolutions + phaseT:nSolutions  + (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refrsesh = 0)
save_summarise_model(randomIntercept, tableName)
randomIntercept
View(dataInput)
randomIntercept = brm(timeSpent ~ phaseT + nSolutions + phaseT:nSolutions  + (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refrsesh = 0)
dataInput$nSolutions
randomIntercept = brm(timeSpent ~ phaseT + nSolutions + phaseT:nSolutions  + (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
#Summary Stats for Decision Problem
dataInput=dataTrial_SAT_Time
timeSummary = dataInput %>% group_by(pID) %>% summarise(acc=mean(timeSpent)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(timeSummary, digits=1, caption = "Time Summary")
yesNoProportions = dataInput %>% group_by(sol,pID) %>% summarise(acc=mean(timeSpent)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(yesNoProportions, digits=1, caption = "Time Spent By Solution")
#Trial (experience effect) effect on timeSpent
dataInput=dataTrial_SAT_Time
dataInput$totalTrial = dataInput$block * dataInput$trial
summaryByBlock = dataInput %>% group_by(block,pID) %>% summarise(time=mean(timeSpent)) %>% summarise(mean=mean(time),min=min(time),max=max(time),SD=sd(time))
kable(summaryByBlock,digits=2 , caption="Time Spent per Trial segregated By Block")
#Regression
# TTTODO
# linearMRandomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + (1|pID),
#                              chains=chains_brms, cores = cores_brms,
#                              seed=seed_brms, refresh = 0,
#                              data=dataOptTime)
linearRandomIntercept = brm(timeSpent | cens(censored_time) ~ totalTrial+ (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
tableName='sat_time_01_r'
save_summarise_model(linearRandomIntercept, tableName)
linearRandomIntercept
#Regression
# TTTODO
# linearMRandomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + (1|pID),
#                              chains=chains_brms, cores = cores_brms,
#                              seed=seed_brms, refresh = 0,
#                              data=dataOptTime)
linearRandomIntercept = brm(timeSpent ~ totalTrial+ (1|pID),#| cens(censored_time)
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
save_summarise_model(linearRandomIntercept, tableName)
linearRandomIntercept
?resp_cens
calculate_tsp_ICexpost =function(opt,max_dist){
ICexpost  =  abs(opt- max_dist)/opt
return(ICexpost)
}
dataTrial_TSP= read_csv2(file.path(data_folder,"TSP_clean.csv"))
dataTrial_TSP_Time = read_csv2(file.path(data_folder,"TSP_clean_time.csv"))
color_scheme_set("purple")
dataTrial_TSP$censored_time = ifelse(dataTrial_TSP$timeSpent == TSPTaskMaxTime,
"right","none")
dataTrial_TSP_Time$censored_time = ifelse(dataTrial_TSP_Time$timeSpent == TSPTaskMaxTime,
"right","none")
instanceProperties_TSP= read_csv2(paste0(data_folder,"instance_properties_TSP.csv"))
dataTrial_TSP = left_join(dataTrial_TSP,instanceProperties_TSP, by = "instanceNumber")
dataTrial_TSP_Time = left_join(dataTrial_TSP_Time,instanceProperties_TSP, by = "instanceNumber")
dataTrial_TSP$ICexpost = calculate_tsp_ICexpost(dataTrial_TSP$dist_from_opt,
dataTrial_TSP$max_distance)
names(instanceProperties_TSP)
names(dataTrial_TSP)
dataTrial_TSP= read_csv2(file.path(data_folder,"TSP_clean.csv"))
dataTrial_TSP_Time = read_csv2(file.path(data_folder,"TSP_clean_time.csv"))
color_scheme_set("purple")
dataTrial_TSP$censored_time = ifelse(dataTrial_TSP$timeSpent == TSPTaskMaxTime,
"right","none")
dataTrial_TSP_Time$censored_time = ifelse(dataTrial_TSP_Time$timeSpent == TSPTaskMaxTime,
"right","none")
instanceProperties_TSP= read_csv2(paste0(data_folder,"instance_properties_TSP.csv"))
instanceProperties_TSP =instanceProperties_TSP %>%
select(instanceNumber,nSolutions,opt_distance,opt_tour,dist_from_opt)
dataTrial_TSP = left_join(dataTrial_TSP,instanceProperties_TSP, by = "instanceNumber")
dataTrial_TSP_Time = left_join(dataTrial_TSP_Time,instanceProperties_TSP, by = "instanceNumber")
dataTrial_TSP$ICexpost = calculate_tsp_ICexpost(dataTrial_TSP$dist_from_opt,
dataTrial_TSP$max_distance)
dataTrial_TSP_Time$ICexpost = calculate_tsp_ICexpost(dataTrial_TSP_Time$dist_from_opt,
dataTrial_TSP_Time$max_distance)
#Summary Stats for Decision Problem
dataInput=dataTrial_TSP
accuracySummary = dataInput %>% group_by(pID) %>% summarise(acc=mean(correct)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(accuracySummary, digits=2, caption = "Accuracy Summary")
answerSummary = dataInput %>% group_by(pID) %>% summarise(acc=mean(answer)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(answerSummary, digits=2, caption = "Proportion of times that YES was selected as the answer")
yesNoProportions = dataInput %>% group_by(sol,pID) %>% summarise(acc=mean(correct)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(yesNoProportions, digits=2, caption = "Accuracy By Solution")
#Summary Stats for Decision Problem
dataInput=dataTrial_TSP_Time
timeSummary = dataInput %>% group_by(pID) %>% summarise(acc=mean(timeSpent)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(timeSummary, digits=1, caption = "Time Summary")
yesNoProportions = dataInput %>% group_by(sol,pID) %>% summarise(acc=mean(timeSpent)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(yesNoProportions, digits=1, caption = "Time Spent By Solution")
#Trial (experience effect) effect on timeSpent
dataInput=dataTrial_TSP_Time
dataInput$totalTrial = dataInput$block * dataInput$trial
summaryByBlock = dataInput %>% group_by(block,pID) %>% summarise(time=mean(timeSpent)) %>% summarise(mean=mean(time),min=min(time),max=max(time),SD=sd(time))
kable(summaryByBlock,digits=2 , caption="Time Spent per Trial segregated By Block")
#Regression
# TTTODO
linearRandomIntercept = brm(timeSpent | cens(censored_time) ~ totalTrial+ (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
tableName='tsp_time_01_r'
save_summarise_model(linearRandomIntercept, tableName)
hist(dataInput$timeSpent)
hist(dataInput$timeSpent,breaks=30)
#Summary Stats for Decision Problem
dataInput=dataTrial_SAT_Time
timeSummary = dataInput %>% group_by(pID) %>% summarise(acc=mean(timeSpent)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(timeSummary, digits=1, caption = "Time Summary")
yesNoProportions = dataInput %>% group_by(sol,pID) %>% summarise(acc=mean(timeSpent)) %>% summarise(mean=mean(acc),min=min(acc),max=max(acc),SD=sd(acc))
kable(yesNoProportions, digits=1, caption = "Time Spent By Solution")
hist(dataInput$timeSpent,breaks=30)
hist(dataInput$timeSpent,breaks=40)
hist(dataInput$timeSpent,breaks=40)
?cens
?resp_cens
linearRandomIntercept = brm(timeSpent ~ totalTrial+ (1|pID),#| cens(censored_time)
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
#Trial (experience effect) effect on timeSpent
dataInput=dataTrial_SAT_Time
dataInput$totalTrial = dataInput$block * dataInput$trial
summaryByBlock = dataInput %>% group_by(block,pID) %>% summarise(time=mean(timeSpent)) %>% summarise(mean=mean(time),min=min(time),max=max(time),SD=sd(time))
kable(summaryByBlock,digits=2 , caption="Time Spent per Trial segregated By Block")
#Regression
# TTTODO
# linearMRandomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + (1|pID),
#                              chains=chains_brms, cores = cores_brms,
#                              seed=seed_brms, refresh = 0,
#                              data=dataOptTime)
linearRandomIntercept = brm(timeSpent ~ totalTrial+ (1|pID),#| cens(censored_time)
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
?brm
linearRandomIntercept = brm(timeSpent ~ totalTrial+ (1|pID),#| cens(censored_time)
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000)
save_summarise_model(linearRandomIntercept, tableName)
dataInput=dataTrial_SAT_Time
#dataInput$phaseT = as.factor(dataInput$phaseT)
dataInput = dataInput %>% filter(sol==1)
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000)
randomIntercept = brm(timeSpent ~ phaseT + nSolutions + phaseT:nSolutions  + (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0)
tableName="sat_time_05_r_A"
save_summarise_model(randomIntercept, tableName)
randomIntercept
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=chains_brms,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000)
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=1,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000)
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=1,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000,
control = list(adapt_delta = 0.98))
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=1,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000,
control = list(adapt_delta = 0.98),
prior = c(uniform(90,140),class= Intercept)))
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=1,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000,
control = list(adapt_delta = 0.98),
prior = c(uniform(90,140),class= Intercept))
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=1,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000,
control = list(adapt_delta = 0.98),
prior = c(normal(90,140),class= Intercept))
#Includes a dummy if nSolutions==0, that's variable: sol.
# TTTODO
randomIntercept = brm(timeSpent | cens(censored_time) ~ phaseT + nSolutions + phaseT:nSolutions + (1|pID),
data=dataInput,
chains=1,
cores = cores_brms,
seed=seed_brms, refresh = 0,
iter=4000,
control = list(adapt_delta = 0.98),
prior = c(prior(normal(90,140),class= Intercept)))
install.packages("rstan")
install.packages("rstan")
install.packages("rstan")
